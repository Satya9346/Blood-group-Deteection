{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4803 images belonging to 8 classes.\n",
      "Found 1197 images belonging to 8 classes.\n",
      "Epoch 1/10\n",
      "960/960 [==============================] - 133s 137ms/step - loss: 0.8299 - accuracy: 0.6890 - val_loss: 0.5225 - val_accuracy: 0.7958\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 104s 108ms/step - loss: 0.3581 - accuracy: 0.8674 - val_loss: 0.3995 - val_accuracy: 0.8469\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 86s 90ms/step - loss: 0.2002 - accuracy: 0.9266 - val_loss: 0.4608 - val_accuracy: 0.8318\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 76s 79ms/step - loss: 0.0966 - accuracy: 0.9687 - val_loss: 0.7015 - val_accuracy: 0.7649\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 68s 71ms/step - loss: 0.0517 - accuracy: 0.9844 - val_loss: 0.4449 - val_accuracy: 0.8536\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 69s 71ms/step - loss: 0.0513 - accuracy: 0.9829 - val_loss: 0.5468 - val_accuracy: 0.8510\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 67s 70ms/step - loss: 0.0216 - accuracy: 0.9940 - val_loss: 0.5904 - val_accuracy: 0.8393\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 68s 71ms/step - loss: 0.0440 - accuracy: 0.9862 - val_loss: 0.6060 - val_accuracy: 0.8151\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 63s 65ms/step - loss: 0.0320 - accuracy: 0.9910 - val_loss: 0.6291 - val_accuracy: 0.8435\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 59s 62ms/step - loss: 0.0384 - accuracy: 0.9865 - val_loss: 0.8949 - val_accuracy: 0.8084\n",
      "240/240 [==============================] - 4s 16ms/step - loss: 0.8935 - accuracy: 0.8087\n",
      "Low Accuracy Model - Loss: 0.8935297131538391, Accuracy: 0.8086884021759033\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# Directory paths\n",
    "train_dir =  r'C:\\Users\\pitta\\Downloads\\blood group\\dataset_blood_group'\n",
    "\n",
    "# Data augmentation and preprocessing for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  # Use 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Train generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Low accuracy model with Input layer\n",
    "def create_low_accuracy_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the low accuracy model\n",
    "low_acc_model = create_low_accuracy_model()\n",
    "\n",
    "# Train the low accuracy model\n",
    "history_low_acc = low_acc_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Evaluate the low accuracy model\n",
    "low_acc_eval = low_acc_model.evaluate(validation_generator)\n",
    "print(f\"Low Accuracy Model - Loss: {low_acc_eval[0]}, Accuracy: {low_acc_eval[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4803 images belonging to 8 classes.\n",
      "Found 1197 images belonging to 8 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94668760/94668760 [==============================] - 8s 0us/step\n",
      "Epoch 1/50\n",
      "150/150 [==============================] - 215s 1s/step - loss: 2.1760 - accuracy: 0.2827 - val_loss: 1.7843 - val_accuracy: 0.3471\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 243s 2s/step - loss: 1.8400 - accuracy: 0.3488 - val_loss: 1.6589 - val_accuracy: 0.3691\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 143s 951ms/step - loss: 1.7355 - accuracy: 0.3599 - val_loss: 1.5180 - val_accuracy: 0.4088\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 131s 875ms/step - loss: 1.6741 - accuracy: 0.3729 - val_loss: 1.5019 - val_accuracy: 0.4003\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 129s 861ms/step - loss: 1.6043 - accuracy: 0.3689 - val_loss: 1.5200 - val_accuracy: 0.4012\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 131s 874ms/step - loss: 1.5776 - accuracy: 0.3811 - val_loss: 1.4510 - val_accuracy: 0.4096\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 144s 963ms/step - loss: 1.5443 - accuracy: 0.4020 - val_loss: 1.3952 - val_accuracy: 0.4535\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 173s 1s/step - loss: 1.5198 - accuracy: 0.4054 - val_loss: 1.4577 - val_accuracy: 0.4147\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 144s 956ms/step - loss: 1.4626 - accuracy: 0.4219 - val_loss: 1.3848 - val_accuracy: 0.4333\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 142s 948ms/step - loss: 1.4715 - accuracy: 0.4091 - val_loss: 1.3943 - val_accuracy: 0.4375\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 146s 977ms/step - loss: 1.4511 - accuracy: 0.4192 - val_loss: 1.3559 - val_accuracy: 0.4468\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 142s 949ms/step - loss: 1.3986 - accuracy: 0.4372 - val_loss: 1.3541 - val_accuracy: 0.4586\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 142s 945ms/step - loss: 1.4089 - accuracy: 0.4391 - val_loss: 1.3462 - val_accuracy: 0.4738\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 140s 932ms/step - loss: 1.3965 - accuracy: 0.4460 - val_loss: 1.3215 - val_accuracy: 0.4814\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 131s 874ms/step - loss: 1.3906 - accuracy: 0.4408 - val_loss: 1.3362 - val_accuracy: 0.4797\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 133s 887ms/step - loss: 1.3624 - accuracy: 0.4515 - val_loss: 1.2896 - val_accuracy: 0.4865\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 139s 925ms/step - loss: 1.3645 - accuracy: 0.4615 - val_loss: 1.3195 - val_accuracy: 0.4814\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 135s 900ms/step - loss: 1.3476 - accuracy: 0.4615 - val_loss: 1.3702 - val_accuracy: 0.4620\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 134s 894ms/step - loss: 1.3449 - accuracy: 0.4597 - val_loss: 1.2893 - val_accuracy: 0.4907\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 136s 907ms/step - loss: 1.3472 - accuracy: 0.4513 - val_loss: 1.3091 - val_accuracy: 0.4519\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 136s 907ms/step - loss: 1.3471 - accuracy: 0.4672 - val_loss: 1.3350 - val_accuracy: 0.4417\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 129s 863ms/step - loss: 1.3335 - accuracy: 0.4615 - val_loss: 1.3096 - val_accuracy: 0.4696\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 134s 895ms/step - loss: 1.3312 - accuracy: 0.4689 - val_loss: 1.2914 - val_accuracy: 0.4764\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 140s 933ms/step - loss: 1.3209 - accuracy: 0.4775 - val_loss: 1.3161 - val_accuracy: 0.4620\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 134s 896ms/step - loss: 1.3263 - accuracy: 0.4716 - val_loss: 1.2625 - val_accuracy: 0.5253\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 137s 911ms/step - loss: 1.3280 - accuracy: 0.4573 - val_loss: 1.2957 - val_accuracy: 0.4848\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 137s 916ms/step - loss: 1.2925 - accuracy: 0.4821 - val_loss: 1.2828 - val_accuracy: 0.4848\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 145s 967ms/step - loss: 1.3110 - accuracy: 0.4873 - val_loss: 1.2918 - val_accuracy: 0.4882\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 145s 968ms/step - loss: 1.2909 - accuracy: 0.4787 - val_loss: 1.2762 - val_accuracy: 0.4932\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 152s 1s/step - loss: 1.2828 - accuracy: 0.4905 - val_loss: 1.3221 - val_accuracy: 0.4789\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 143s 953ms/step - loss: 1.3010 - accuracy: 0.4768 - val_loss: 1.2630 - val_accuracy: 0.4975\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 139s 927ms/step - loss: 1.2927 - accuracy: 0.4934 - val_loss: 1.2635 - val_accuracy: 0.4882\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 137s 911ms/step - loss: 1.2790 - accuracy: 0.4823 - val_loss: 1.2986 - val_accuracy: 0.4535\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 138s 919ms/step - loss: 1.2726 - accuracy: 0.4877 - val_loss: 1.2490 - val_accuracy: 0.5017\n",
      "Epoch 35/50\n",
      "150/150 [==============================] - 136s 903ms/step - loss: 1.2695 - accuracy: 0.4940 - val_loss: 1.2574 - val_accuracy: 0.5372\n",
      "Epoch 36/50\n",
      "150/150 [==============================] - 137s 915ms/step - loss: 1.2710 - accuracy: 0.4886 - val_loss: 1.2586 - val_accuracy: 0.5008\n",
      "Epoch 37/50\n",
      "150/150 [==============================] - 136s 910ms/step - loss: 1.2569 - accuracy: 0.4976 - val_loss: 1.2278 - val_accuracy: 0.5118\n",
      "Epoch 38/50\n",
      "150/150 [==============================] - 133s 889ms/step - loss: 1.2509 - accuracy: 0.4995 - val_loss: 1.2344 - val_accuracy: 0.5059\n",
      "Epoch 39/50\n",
      "150/150 [==============================] - 144s 961ms/step - loss: 1.2490 - accuracy: 0.4968 - val_loss: 1.2587 - val_accuracy: 0.4916\n",
      "Epoch 40/50\n",
      "150/150 [==============================] - 150s 998ms/step - loss: 1.2715 - accuracy: 0.4882 - val_loss: 1.2584 - val_accuracy: 0.4907\n",
      "Epoch 41/50\n",
      "150/150 [==============================] - 143s 952ms/step - loss: 1.2369 - accuracy: 0.5068 - val_loss: 1.2608 - val_accuracy: 0.5051\n",
      "Epoch 42/50\n",
      "150/150 [==============================] - 151s 1s/step - loss: 1.2579 - accuracy: 0.5032 - val_loss: 1.2424 - val_accuracy: 0.5152\n",
      "Epoch 43/50\n",
      "150/150 [==============================] - 162s 1s/step - loss: 1.2415 - accuracy: 0.5032 - val_loss: 1.2138 - val_accuracy: 0.5194\n",
      "Epoch 44/50\n",
      "150/150 [==============================] - 134s 890ms/step - loss: 1.2336 - accuracy: 0.5062 - val_loss: 1.2481 - val_accuracy: 0.4975\n",
      "Epoch 45/50\n",
      "150/150 [==============================] - 122s 813ms/step - loss: 1.2498 - accuracy: 0.4980 - val_loss: 1.2151 - val_accuracy: 0.5321\n",
      "Epoch 46/50\n",
      "150/150 [==============================] - 122s 814ms/step - loss: 1.2328 - accuracy: 0.5051 - val_loss: 1.2429 - val_accuracy: 0.4907\n",
      "Epoch 47/50\n",
      "150/150 [==============================] - 118s 788ms/step - loss: 1.2496 - accuracy: 0.5009 - val_loss: 1.2759 - val_accuracy: 0.5042\n",
      "Epoch 48/50\n",
      "150/150 [==============================] - 124s 828ms/step - loss: 1.2298 - accuracy: 0.5139 - val_loss: 1.2000 - val_accuracy: 0.5110\n",
      "Epoch 49/50\n",
      "150/150 [==============================] - 126s 840ms/step - loss: 1.2355 - accuracy: 0.5135 - val_loss: 1.2085 - val_accuracy: 0.5346\n",
      "Epoch 50/50\n",
      "150/150 [==============================] - 127s 844ms/step - loss: 1.2134 - accuracy: 0.5154 - val_loss: 1.1805 - val_accuracy: 0.5220\n",
      "Epoch 1/30\n",
      "150/150 [==============================] - 215s 1s/step - loss: 1.4193 - accuracy: 0.4573 - val_loss: 1.3034 - val_accuracy: 0.4738\n",
      "Epoch 2/30\n",
      "150/150 [==============================] - 221s 1s/step - loss: 1.2400 - accuracy: 0.5165 - val_loss: 1.1933 - val_accuracy: 0.5456\n",
      "Epoch 3/30\n",
      "150/150 [==============================] - 200s 1s/step - loss: 1.1377 - accuracy: 0.5515 - val_loss: 1.1277 - val_accuracy: 0.5608\n",
      "Epoch 4/30\n",
      "150/150 [==============================] - 201s 1s/step - loss: 1.0765 - accuracy: 0.5835 - val_loss: 1.0649 - val_accuracy: 0.5785\n",
      "Epoch 5/30\n",
      "150/150 [==============================] - 196s 1s/step - loss: 1.0203 - accuracy: 0.6074 - val_loss: 0.9875 - val_accuracy: 0.6182\n",
      "Epoch 6/30\n",
      "150/150 [==============================] - 194s 1s/step - loss: 0.9764 - accuracy: 0.6305 - val_loss: 0.9823 - val_accuracy: 0.6318\n",
      "Epoch 7/30\n",
      "150/150 [==============================] - 198s 1s/step - loss: 0.9423 - accuracy: 0.6456 - val_loss: 0.9710 - val_accuracy: 0.6174\n",
      "Epoch 8/30\n",
      "150/150 [==============================] - 209s 1s/step - loss: 0.8901 - accuracy: 0.6531 - val_loss: 0.8748 - val_accuracy: 0.6622\n",
      "Epoch 9/30\n",
      "150/150 [==============================] - 201s 1s/step - loss: 0.8411 - accuracy: 0.6837 - val_loss: 0.8150 - val_accuracy: 0.6883\n",
      "Epoch 10/30\n",
      "150/150 [==============================] - 178s 1s/step - loss: 0.8584 - accuracy: 0.6787 - val_loss: 0.8550 - val_accuracy: 0.6791\n",
      "Epoch 11/30\n",
      "150/150 [==============================] - 180s 1s/step - loss: 0.8463 - accuracy: 0.6816 - val_loss: 0.7636 - val_accuracy: 0.7103\n",
      "Epoch 12/30\n",
      "150/150 [==============================] - 176s 1s/step - loss: 0.8004 - accuracy: 0.7005 - val_loss: 0.8099 - val_accuracy: 0.6816\n",
      "Epoch 13/30\n",
      "150/150 [==============================] - 177s 1s/step - loss: 0.7619 - accuracy: 0.7101 - val_loss: 0.7779 - val_accuracy: 0.7035\n",
      "Epoch 14/30\n",
      "150/150 [==============================] - 177s 1s/step - loss: 0.7696 - accuracy: 0.7055 - val_loss: 0.8413 - val_accuracy: 0.6858\n",
      "Epoch 15/30\n",
      "150/150 [==============================] - 176s 1s/step - loss: 0.7752 - accuracy: 0.7061 - val_loss: 0.8096 - val_accuracy: 0.7027\n",
      "Epoch 16/30\n",
      "150/150 [==============================] - 175s 1s/step - loss: 0.7740 - accuracy: 0.7108 - val_loss: 0.7733 - val_accuracy: 0.7120\n",
      "Epoch 17/30\n",
      "150/150 [==============================] - 175s 1s/step - loss: 0.7603 - accuracy: 0.7141 - val_loss: 0.7670 - val_accuracy: 0.7137\n",
      "Epoch 18/30\n",
      "150/150 [==============================] - 177s 1s/step - loss: 0.7128 - accuracy: 0.7296 - val_loss: 0.7388 - val_accuracy: 0.7128\n",
      "Epoch 19/30\n",
      "150/150 [==============================] - 175s 1s/step - loss: 0.7182 - accuracy: 0.7315 - val_loss: 0.6915 - val_accuracy: 0.7492\n",
      "Epoch 20/30\n",
      "150/150 [==============================] - 176s 1s/step - loss: 0.7273 - accuracy: 0.7252 - val_loss: 0.6950 - val_accuracy: 0.7297\n",
      "Epoch 21/30\n",
      "150/150 [==============================] - 176s 1s/step - loss: 0.7105 - accuracy: 0.7294 - val_loss: 0.6886 - val_accuracy: 0.7356\n",
      "Epoch 22/30\n",
      "150/150 [==============================] - 174s 1s/step - loss: 0.6800 - accuracy: 0.7399 - val_loss: 0.7177 - val_accuracy: 0.7145\n",
      "Epoch 23/30\n",
      "150/150 [==============================] - 176s 1s/step - loss: 0.6746 - accuracy: 0.7479 - val_loss: 0.6995 - val_accuracy: 0.7331\n",
      "Epoch 24/30\n",
      "150/150 [==============================] - 175s 1s/step - loss: 0.6977 - accuracy: 0.7357 - val_loss: 0.7497 - val_accuracy: 0.6883\n",
      "Epoch 25/30\n",
      "150/150 [==============================] - 175s 1s/step - loss: 0.6854 - accuracy: 0.7365 - val_loss: 0.6602 - val_accuracy: 0.7627\n",
      "Epoch 26/30\n",
      "150/150 [==============================] - 175s 1s/step - loss: 0.6703 - accuracy: 0.7422 - val_loss: 0.6917 - val_accuracy: 0.7221\n",
      "Epoch 27/30\n",
      "150/150 [==============================] - 173s 1s/step - loss: 0.6648 - accuracy: 0.7485 - val_loss: 0.6631 - val_accuracy: 0.7382\n",
      "Epoch 28/30\n",
      "150/150 [==============================] - 175s 1s/step - loss: 0.6642 - accuracy: 0.7541 - val_loss: 0.6903 - val_accuracy: 0.7289\n",
      "Epoch 29/30\n",
      "150/150 [==============================] - 174s 1s/step - loss: 0.6360 - accuracy: 0.7552 - val_loss: 0.6250 - val_accuracy: 0.7551\n",
      "Epoch 30/30\n",
      "150/150 [==============================] - 174s 1s/step - loss: 0.6632 - accuracy: 0.7479 - val_loss: 0.6638 - val_accuracy: 0.7314\n",
      "38/38 [==============================] - 22s 583ms/step - loss: 0.6665 - accuracy: 0.7544\n",
      "Fine-Tuned Model - Loss: 0.6665336489677429, Accuracy: 0.7543859481811523\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 128  # Increased resolution\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Directory paths\n",
    "train_dir = r'C:\\Users\\pitta\\Downloads\\blood group\\dataset_blood_group'\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,  \n",
    "    width_shift_range=0.2,  \n",
    "    height_shift_range=0.2,  \n",
    "    shear_range=0.2,  \n",
    "    zoom_range=0.2,  \n",
    "    horizontal_flip=True,  \n",
    "    validation_split=0.2  \n",
    ")\n",
    "\n",
    "# Train Generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Validation Generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load Pretrained ResNet50V2 Model\n",
    "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Freeze Base Layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add Custom Layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=50,  \n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Unfreeze some ResNet50V2 layers for fine-tuning\n",
    "for layer in base_model.layers[-20:]:  # Unfreeze last 20 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-Tuning\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=30,  \n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "final_eval = model.evaluate(validation_generator)\n",
    "print(f\"Fine-Tuned Model - Loss: {final_eval[0]}, Accuracy: {final_eval[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4803 images belonging to 8 classes.\n",
      "Found 1197 images belonging to 8 classes.\n",
      "Epoch 1/16\n",
      "150/150 [==============================] - 17s 109ms/step - loss: 1.4826 - accuracy: 0.4320 - val_loss: 0.7638 - val_accuracy: 0.7103\n",
      "Epoch 2/16\n",
      "150/150 [==============================] - 15s 102ms/step - loss: 0.6756 - accuracy: 0.7550 - val_loss: 0.4676 - val_accuracy: 0.8311\n",
      "Epoch 3/16\n",
      "150/150 [==============================] - 15s 102ms/step - loss: 0.5089 - accuracy: 0.8120 - val_loss: 0.4265 - val_accuracy: 0.8412\n",
      "Epoch 4/16\n",
      "150/150 [==============================] - 15s 101ms/step - loss: 0.4319 - accuracy: 0.8376 - val_loss: 0.4085 - val_accuracy: 0.8412\n",
      "Epoch 5/16\n",
      "150/150 [==============================] - 17s 113ms/step - loss: 0.3803 - accuracy: 0.8541 - val_loss: 0.6439 - val_accuracy: 0.7720\n",
      "Epoch 6/16\n",
      "150/150 [==============================] - 16s 108ms/step - loss: 0.3882 - accuracy: 0.8537 - val_loss: 0.3975 - val_accuracy: 0.8590\n",
      "Epoch 7/16\n",
      "150/150 [==============================] - 16s 108ms/step - loss: 0.3426 - accuracy: 0.8680 - val_loss: 0.4838 - val_accuracy: 0.8015\n",
      "Epoch 8/16\n",
      "150/150 [==============================] - 16s 108ms/step - loss: 0.2978 - accuracy: 0.8835 - val_loss: 0.3395 - val_accuracy: 0.8742\n",
      "Epoch 9/16\n",
      "150/150 [==============================] - 16s 109ms/step - loss: 0.2834 - accuracy: 0.8914 - val_loss: 0.3170 - val_accuracy: 0.8801\n",
      "Epoch 10/16\n",
      "150/150 [==============================] - 17s 113ms/step - loss: 0.2742 - accuracy: 0.8977 - val_loss: 0.3538 - val_accuracy: 0.8716\n",
      "Epoch 11/16\n",
      "150/150 [==============================] - 17s 112ms/step - loss: 0.2254 - accuracy: 0.9118 - val_loss: 0.3796 - val_accuracy: 0.8640\n",
      "Epoch 12/16\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 0.2108 - accuracy: 0.9214 - val_loss: 0.4320 - val_accuracy: 0.8514\n",
      "Epoch 13/16\n",
      "150/150 [==============================] - 16s 108ms/step - loss: 0.1976 - accuracy: 0.9250 - val_loss: 0.3108 - val_accuracy: 0.8868\n",
      "Epoch 14/16\n",
      "150/150 [==============================] - 17s 110ms/step - loss: 0.1684 - accuracy: 0.9371 - val_loss: 0.4061 - val_accuracy: 0.8581\n",
      "Epoch 15/16\n",
      "150/150 [==============================] - 16s 109ms/step - loss: 0.1389 - accuracy: 0.9480 - val_loss: 0.3655 - val_accuracy: 0.8818\n",
      "Epoch 16/16\n",
      "150/150 [==============================] - 16s 108ms/step - loss: 0.1520 - accuracy: 0.9451 - val_loss: 0.4249 - val_accuracy: 0.8522\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 0.4221 - accuracy: 0.8530\n",
      "High Accuracy Model - Loss: 0.42208224534988403, Accuracy: 0.852965772151947\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Directory paths (adjust as needed)\n",
    "train_dir = r'C:\\Users\\pitta\\Downloads\\blood group\\dataset_blood_group'\n",
    "\n",
    "# Data augmentation and preprocessing for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0,1]\n",
    "    validation_split=0.2  # Use 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Train generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Training data subset\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Validation data subset\n",
    ")\n",
    "\n",
    "# High accuracy model: Deep CNN\n",
    "def create_high_accuracy_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.55),  # Regularization to prevent overfitting\n",
    "        tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "high_acc_model = create_high_accuracy_model()\n",
    "\n",
    "# Train the model\n",
    "history_high_acc = high_acc_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=16,  # Adjust the number of epochs based on your preference\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "high_acc_eval = high_acc_model.evaluate(validation_generator)\n",
    "print(f\"High Accuracy Model - Loss: {high_acc_eval[0]}, Accuracy: {high_acc_eval[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to blood_group_model.h5\n"
     ]
    }
   ],
   "source": [
    "h5_path = 'blood_group_model.h5'\n",
    "high_acc_model.save(h5_path)\n",
    "print(f\"Model saved to {h5_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4803 images belonging to 8 classes.\n",
      "Found 1197 images belonging to 8 classes.\n",
      "Epoch 1/18\n",
      "133/133 [==============================] - 20s 144ms/step - loss: 1.3103 - accuracy: 0.4949 - val_loss: 0.6295 - val_accuracy: 0.7584\n",
      "Epoch 2/18\n",
      "133/133 [==============================] - 17s 126ms/step - loss: 0.5633 - accuracy: 0.7894 - val_loss: 0.4471 - val_accuracy: 0.8384\n",
      "Epoch 3/18\n",
      "133/133 [==============================] - 15s 109ms/step - loss: 0.4919 - accuracy: 0.8137 - val_loss: 0.6099 - val_accuracy: 0.7643\n",
      "Epoch 4/18\n",
      "133/133 [==============================] - 16s 123ms/step - loss: 0.4141 - accuracy: 0.8481 - val_loss: 0.4875 - val_accuracy: 0.8266\n",
      "Epoch 5/18\n",
      "133/133 [==============================] - 16s 118ms/step - loss: 0.3865 - accuracy: 0.8511 - val_loss: 0.4407 - val_accuracy: 0.8367\n",
      "Epoch 6/18\n",
      "133/133 [==============================] - 17s 124ms/step - loss: 0.3595 - accuracy: 0.8689 - val_loss: 0.3675 - val_accuracy: 0.8729\n",
      "Epoch 7/18\n",
      "133/133 [==============================] - 16s 117ms/step - loss: 0.3051 - accuracy: 0.8848 - val_loss: 0.3662 - val_accuracy: 0.8586\n",
      "Epoch 8/18\n",
      "133/133 [==============================] - 17s 125ms/step - loss: 0.2736 - accuracy: 0.8888 - val_loss: 0.6331 - val_accuracy: 0.7912\n",
      "Epoch 9/18\n",
      "133/133 [==============================] - 15s 115ms/step - loss: 0.2990 - accuracy: 0.8853 - val_loss: 0.4836 - val_accuracy: 0.8258\n",
      "Epoch 10/18\n",
      "133/133 [==============================] - 15s 113ms/step - loss: 0.2434 - accuracy: 0.9069 - val_loss: 0.4699 - val_accuracy: 0.8274\n",
      "Epoch 11/18\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.2207 - accuracy: 0.9127 - val_loss: 0.3766 - val_accuracy: 0.8695\n",
      "Epoch 12/18\n",
      "133/133 [==============================] - 15s 113ms/step - loss: 0.1883 - accuracy: 0.9304 - val_loss: 0.3270 - val_accuracy: 0.8695\n",
      "Epoch 13/18\n",
      "133/133 [==============================] - 15s 114ms/step - loss: 0.1711 - accuracy: 0.9364 - val_loss: 0.3514 - val_accuracy: 0.8737\n",
      "Epoch 14/18\n",
      "133/133 [==============================] - 15s 114ms/step - loss: 0.1384 - accuracy: 0.9486 - val_loss: 0.4164 - val_accuracy: 0.8687\n",
      "Epoch 15/18\n",
      "133/133 [==============================] - 15s 115ms/step - loss: 0.1379 - accuracy: 0.9480 - val_loss: 0.4797 - val_accuracy: 0.8426\n",
      "Epoch 16/18\n",
      "133/133 [==============================] - 16s 117ms/step - loss: 0.1344 - accuracy: 0.9499 - val_loss: 0.3955 - val_accuracy: 0.8678\n",
      "Epoch 17/18\n",
      "133/133 [==============================] - 16s 119ms/step - loss: 0.1073 - accuracy: 0.9625 - val_loss: 0.4042 - val_accuracy: 0.8796\n",
      "Epoch 18/18\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.0871 - accuracy: 0.9717 - val_loss: 0.3705 - val_accuracy: 0.8897\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 0.3681 - accuracy: 0.8906\n",
      "High Accuracy Model - Loss: 0.36814603209495544, Accuracy: 0.890559732913971\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 36\n",
    "\n",
    "# Directory paths (adjust as needed)\n",
    "train_dir = r'C:\\Users\\pitta\\Downloads\\blood group\\dataset_blood_group'\n",
    "\n",
    "# Data augmentation and preprocessing for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0,1]\n",
    "    validation_split=0.2  # Use 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Train generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Training data subset\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Validation data subset\n",
    ")\n",
    "\n",
    "# High accuracy model: Deep CNN\n",
    "def create_high_accuracy_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.55),  # Regularization to prevent overfitting\n",
    "        tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "high_acc_model = create_high_accuracy_model()\n",
    "\n",
    "# Train the model\n",
    "history_high_acc = high_acc_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=18,  # Adjust the number of epochs based on your preference\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "high_acc_eval = high_acc_model.evaluate(validation_generator)\n",
    "print(f\"High Accuracy Model - Loss: {high_acc_eval[0]}, Accuracy: {high_acc_eval[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4803 images belonging to 8 classes.\n",
      "Found 1197 images belonging to 8 classes.\n",
      "Epoch 1/20\n",
      "133/133 [==============================] - 16s 115ms/step - loss: 1.4738 - accuracy: 0.4237 - val_loss: 0.7223 - val_accuracy: 0.7273\n",
      "Epoch 2/20\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.6372 - accuracy: 0.7661 - val_loss: 0.5436 - val_accuracy: 0.7955\n",
      "Epoch 3/20\n",
      "133/133 [==============================] - 16s 124ms/step - loss: 0.4999 - accuracy: 0.8164 - val_loss: 0.4589 - val_accuracy: 0.8274\n",
      "Epoch 4/20\n",
      "133/133 [==============================] - 16s 120ms/step - loss: 0.4560 - accuracy: 0.8255 - val_loss: 0.4683 - val_accuracy: 0.8131\n",
      "Epoch 5/20\n",
      "133/133 [==============================] - 16s 119ms/step - loss: 0.3691 - accuracy: 0.8657 - val_loss: 0.3902 - val_accuracy: 0.8586\n",
      "Epoch 6/20\n",
      "133/133 [==============================] - 16s 120ms/step - loss: 0.3677 - accuracy: 0.8632 - val_loss: 0.3365 - val_accuracy: 0.8704\n",
      "Epoch 7/20\n",
      "133/133 [==============================] - 16s 120ms/step - loss: 0.3426 - accuracy: 0.8733 - val_loss: 0.3959 - val_accuracy: 0.8569\n",
      "Epoch 8/20\n",
      "133/133 [==============================] - 16s 118ms/step - loss: 0.2964 - accuracy: 0.8850 - val_loss: 0.3478 - val_accuracy: 0.8670\n",
      "Epoch 9/20\n",
      "133/133 [==============================] - 16s 119ms/step - loss: 0.2877 - accuracy: 0.8922 - val_loss: 0.3412 - val_accuracy: 0.8729\n",
      "Epoch 10/20\n",
      "133/133 [==============================] - 16s 120ms/step - loss: 0.2527 - accuracy: 0.9046 - val_loss: 0.3995 - val_accuracy: 0.8552\n",
      "Epoch 11/20\n",
      "133/133 [==============================] - 17s 124ms/step - loss: 0.2527 - accuracy: 0.9058 - val_loss: 0.3371 - val_accuracy: 0.8737\n",
      "Epoch 12/20\n",
      "133/133 [==============================] - 16s 118ms/step - loss: 0.2424 - accuracy: 0.9081 - val_loss: 0.3499 - val_accuracy: 0.8620\n",
      "Epoch 13/20\n",
      "133/133 [==============================] - 16s 118ms/step - loss: 0.2108 - accuracy: 0.9186 - val_loss: 0.3306 - val_accuracy: 0.8763\n",
      "Epoch 14/20\n",
      "133/133 [==============================] - 16s 117ms/step - loss: 0.1842 - accuracy: 0.9264 - val_loss: 0.3704 - val_accuracy: 0.8695\n",
      "Epoch 15/20\n",
      "133/133 [==============================] - 16s 118ms/step - loss: 0.1806 - accuracy: 0.9325 - val_loss: 0.4445 - val_accuracy: 0.8392\n",
      "Epoch 16/20\n",
      "133/133 [==============================] - 16s 118ms/step - loss: 0.1722 - accuracy: 0.9295 - val_loss: 0.3710 - val_accuracy: 0.8721\n",
      "Epoch 17/20\n",
      "133/133 [==============================] - 16s 117ms/step - loss: 0.1273 - accuracy: 0.9526 - val_loss: 0.4125 - val_accuracy: 0.8754\n",
      "Epoch 18/20\n",
      "133/133 [==============================] - 16s 118ms/step - loss: 0.1257 - accuracy: 0.9526 - val_loss: 0.3832 - val_accuracy: 0.8712\n",
      "Epoch 19/20\n",
      "133/133 [==============================] - 16s 117ms/step - loss: 0.0882 - accuracy: 0.9679 - val_loss: 0.3805 - val_accuracy: 0.8855\n",
      "Epoch 20/20\n",
      "133/133 [==============================] - 16s 118ms/step - loss: 0.0886 - accuracy: 0.9685 - val_loss: 0.4672 - val_accuracy: 0.8712\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.4680 - accuracy: 0.8713\n",
      "High Accuracy Model - Loss: 0.46796515583992004, Accuracy: 0.871345043182373\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 36\n",
    "\n",
    "# Directory paths (adjust as needed)\n",
    "train_dir = r'C:\\Users\\pitta\\Downloads\\blood group\\dataset_blood_group'\n",
    "\n",
    "# Data augmentation and preprocessing for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0,1]\n",
    "    validation_split=0.2  # Use 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Train generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Training data subset\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Validation data subset\n",
    ")\n",
    "\n",
    "# High accuracy model: Deep CNN\n",
    "def create_high_accuracy_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.55),  # Regularization to prevent overfitting\n",
    "        tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "high_acc_model = create_high_accuracy_model()\n",
    "\n",
    "# Train the model\n",
    "history_high_acc = high_acc_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,  # Adjust the number of epochs based on your preference\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "high_acc_eval = high_acc_model.evaluate(validation_generator)\n",
    "print(f\"High Accuracy Model - Loss: {high_acc_eval[0]}, Accuracy: {high_acc_eval[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4803 images belonging to 8 classes.\n",
      "Found 1197 images belonging to 8 classes.\n",
      "Epoch 1/25\n",
      "133/133 [==============================] - 16s 116ms/step - loss: 1.3040 - accuracy: 0.5026 - val_loss: 0.6453 - val_accuracy: 0.7483\n",
      "Epoch 2/25\n",
      "133/133 [==============================] - 15s 111ms/step - loss: 0.6363 - accuracy: 0.7592 - val_loss: 0.7837 - val_accuracy: 0.7121\n",
      "Epoch 3/25\n",
      "133/133 [==============================] - 15s 111ms/step - loss: 0.5102 - accuracy: 0.8091 - val_loss: 0.4476 - val_accuracy: 0.8333\n",
      "Epoch 4/25\n",
      "133/133 [==============================] - 15s 113ms/step - loss: 0.4239 - accuracy: 0.8410 - val_loss: 0.4197 - val_accuracy: 0.8451\n",
      "Epoch 5/25\n",
      "133/133 [==============================] - 15s 112ms/step - loss: 0.3869 - accuracy: 0.8555 - val_loss: 0.4434 - val_accuracy: 0.8224\n",
      "Epoch 6/25\n",
      "133/133 [==============================] - 15s 113ms/step - loss: 0.3594 - accuracy: 0.8639 - val_loss: 0.3799 - val_accuracy: 0.8502\n",
      "Epoch 7/25\n",
      "133/133 [==============================] - 15s 114ms/step - loss: 0.3235 - accuracy: 0.8787 - val_loss: 0.3743 - val_accuracy: 0.8519\n",
      "Epoch 8/25\n",
      "133/133 [==============================] - 15s 115ms/step - loss: 0.3054 - accuracy: 0.8811 - val_loss: 0.3609 - val_accuracy: 0.8594\n",
      "Epoch 9/25\n",
      "133/133 [==============================] - 15s 114ms/step - loss: 0.2862 - accuracy: 0.8947 - val_loss: 0.3385 - val_accuracy: 0.8721\n",
      "Epoch 10/25\n",
      "133/133 [==============================] - 16s 117ms/step - loss: 0.2677 - accuracy: 0.9006 - val_loss: 0.3506 - val_accuracy: 0.8763\n",
      "Epoch 11/25\n",
      "133/133 [==============================] - 15s 115ms/step - loss: 0.2462 - accuracy: 0.9079 - val_loss: 0.3960 - val_accuracy: 0.8502\n",
      "Epoch 12/25\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.2312 - accuracy: 0.9087 - val_loss: 0.3511 - val_accuracy: 0.8721\n",
      "Epoch 13/25\n",
      "133/133 [==============================] - 15s 114ms/step - loss: 0.2158 - accuracy: 0.9180 - val_loss: 0.4808 - val_accuracy: 0.8232\n",
      "Epoch 14/25\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.2001 - accuracy: 0.9236 - val_loss: 0.3762 - val_accuracy: 0.8704\n",
      "Epoch 15/25\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.1877 - accuracy: 0.9295 - val_loss: 0.3670 - val_accuracy: 0.8645\n",
      "Epoch 16/25\n",
      "133/133 [==============================] - 16s 119ms/step - loss: 0.1517 - accuracy: 0.9415 - val_loss: 0.3449 - val_accuracy: 0.8737\n",
      "Epoch 17/25\n",
      "133/133 [==============================] - 16s 117ms/step - loss: 0.1504 - accuracy: 0.9465 - val_loss: 0.3659 - val_accuracy: 0.8805\n",
      "Epoch 18/25\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.1159 - accuracy: 0.9545 - val_loss: 0.3871 - val_accuracy: 0.8687\n",
      "Epoch 19/25\n",
      "133/133 [==============================] - 15s 115ms/step - loss: 0.1216 - accuracy: 0.9530 - val_loss: 0.4128 - val_accuracy: 0.8636\n",
      "Epoch 20/25\n",
      "133/133 [==============================] - 16s 116ms/step - loss: 0.1390 - accuracy: 0.9471 - val_loss: 0.4181 - val_accuracy: 0.8586\n",
      "Epoch 21/25\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.0846 - accuracy: 0.9706 - val_loss: 0.3725 - val_accuracy: 0.8763\n",
      "Epoch 22/25\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.1040 - accuracy: 0.9627 - val_loss: 0.4436 - val_accuracy: 0.8678\n",
      "Epoch 23/25\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.0858 - accuracy: 0.9698 - val_loss: 0.5194 - val_accuracy: 0.8510\n",
      "Epoch 24/25\n",
      "133/133 [==============================] - 16s 117ms/step - loss: 0.0734 - accuracy: 0.9731 - val_loss: 0.4503 - val_accuracy: 0.8569\n",
      "Epoch 25/25\n",
      "133/133 [==============================] - 15s 116ms/step - loss: 0.0418 - accuracy: 0.9853 - val_loss: 0.5007 - val_accuracy: 0.8678\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 0.5067 - accuracy: 0.8672\n",
      "High Accuracy Model - Loss: 0.5066794753074646, Accuracy: 0.8671678900718689\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 36\n",
    "\n",
    "# Directory paths (adjust as needed)\n",
    "train_dir = r'C:\\Users\\pitta\\Downloads\\blood group\\dataset_blood_group'\n",
    "\n",
    "# Data augmentation and preprocessing for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0,1]\n",
    "    validation_split=0.2  # Use 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Train generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Training data subset\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Validation data subset\n",
    ")\n",
    "\n",
    "# High accuracy model: Deep CNN\n",
    "def create_high_accuracy_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.55),  # Regularization to prevent overfitting\n",
    "        tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "high_acc_model = create_high_accuracy_model()\n",
    "\n",
    "# Train the model\n",
    "history_high_acc = high_acc_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=25,  # Adjust the number of epochs based on your preference\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "high_acc_eval = high_acc_model.evaluate(validation_generator)\n",
    "print(f\"High Accuracy Model - Loss: {high_acc_eval[0]}, Accuracy: {high_acc_eval[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
